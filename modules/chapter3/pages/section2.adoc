= Performing Migration LAB

- Prior to initiating the migration, validate that the source environment is stable and operational, and perform a full backup of the application.

video::container_migration.mp4[align="left",width=800,height=500]

The above video is at 1.75X seed to demostrate the below steps.

The first essential step is to halt the services so we can perform the restore. As we have perfomed the Installation of all the comonenets on a single systems itslef so we are going to perfom all the below steps on the `container.lab` system only but you might be having different systems so highlighting which command needs to run on which node: 

.. Access the Automation Controller node and run:
+
[source,bash,role=execute]
----
systemctl --user stop automation-controller-task automation-controller-web automation-controller-rsyslog
systemctl --user stop receptor
----

.. Access the Automation Hub node and run:
+
[source,bash,role=execute]
----
systemctl --user stop automation-hub-api automation-hub-content automation-hub-web automation-hub-worker-1 automation-hub-worker-2
----

.. Access the Automation Gateway node and run:
+
[source,bash,role=execute]
----
systemctl --user stop automation-gateway automation-gateway-proxy
----

.. Access the Automation Gateway node when using standalone redis, or all nodes from the redis group in your inventory when using clustered redis and run:
+
[source,bash,role=execute]
----
systemctl --user stop redis-unix redis-tcp
----

Note: In an enterprise deployment, the components will be running on different nodes. Execute the commands on the respective component node.

To perform a restore on an installer-managed database, you'll need to use a temporary container to run the psql and pg_restore commands. This container must be created from the database node.

The following command starts a temporary container named postgresql_restore_temp and opens a shell inside it. This command also mounts the necessary PostgreSQL certificates and the restore artifact.

- Execute the commands in the below order: 
+
[source,bash,role=execute]
----
tar xf migration_artifact.tar
----
+
[source,bash,role=execute]
----
podman run -it --rm --name postgresql_restore_temp --network host --volume ~/aap/tls/extracted:/etc/pki/ca-trust/extracted:z --volume ~/aap/postgresql/server.crt:/var/lib/pgsql/server.crt:ro,z --volume ~/aap/postgresql/server.key:/var/lib/pgsql/server.key:ro,z --volume ~/migration_artifact:/var/lib/pgsql/backups:ro,z registry.redhat.io/rhel9/postgresql-15:latest bash
---- 

Once executed, this command will provide you with a shell inside the container, where your restore artifact is accessible at /var/lib/pgsql/backups.

*Create a DB role* 

- When inside the container, access the database and ensure the users have the CREATEDB role.
+
[source,bash,role=execute]
----
psql -h localhost -U postgres
---- 

- For each component name, add the CREATEDB role to the Owner.
+
[source,bash,role=execute]
----
postgres=# ALTER ROLE awx WITH CREATEDB;
postgres=# \q
----

- With the CREATEDB in place, access the path where the artifact is mounted, and you may run the `pg_restore` commands.
+
[source,bash,role=execute]
----
cd /var/lib/pgsql/backups/
sha256sum --check sha256sum.txt
----
+
[source,bash,role=execute]
----
pg_restore --clean --create --no-owner -h localhost -U awx -d template1 controller/automationcontroller.pgc
----

Note: 
The above restore process can be followed for the other components like Unified UI or Automation hub or EDA. Currently we have show for controller only.  

- After the restore, remove the permissions from the user
+
[source,bash,role=execute]
----
psql -h localhost -U postgres
---- 
+
[source,bash,role=execute]
----
postgres=# ALTER ROLE awx WITH NOCREATEDB;
postgres=# \q
----
+ 
[source,bash,role=execute]
----
exit
----

- All the below services needs to be started again on all the nodes: 

.. Access the Automation Controller node and run:
+ 
[source,bash,role=execute]
----
systemctl --user start automation-controller-task automation-controller-web automation-controller-rsyslog
systemctl --user start receptor
----

.. Access the Automation Hub node and run:
+ 
[source,bash,role=execute]
----
systemctl --user start automation-hub-api automation-hub-content automation-hub-web automation-hub-worker-1 automation-hub-worker-2
----

.. Access the Automation Gateway node and run:
+ 
[source,bash,role=execute]
----
systemctl --user start automation-gateway automation-gateway-proxy
---- 

.. Access the Automation Gateway node when using standalone redis, or all nodes from the redis group in your inventory when using clustered redis and run:
+ 
[source,bash,role=execute]
----
systemctl --user start redis-unix redis-tcp
----

*Target Environment Post-Import Reconciliation*


- Deprovision gateway configuration SSH to the host serving an automation-gateway container as the same rootless user from 4.2.6 and run the following to remove the automation gateway proxy configuration:
+ 
[source,bash,role=execute]
----
podman exec -it automation-gateway bash
aap-gateway-manage migrate
----
+ 
[source,bash,role=execute]
----
aap-gateway-manage shell_plus
----
+ 
[source,bash,role=execute]
----
>>> HTTPPort.objects.all().delete(); ServiceNode.objects.all().delete();
ServiceCluster.objects.all().delete()
>>> CTRL+D <-- to exit
---- 
+ 
[source,bash,role=execute]
----
exit
----

- Re-run the containerized installer on the target environment using the same inventory from the installation.

- Validate Instances for Automation Execution SSH to the host serving an automation-controller-task container as the same rootless user from above and run the following to validate and/or remove instances which are orphaned from the source artifact:
+ 
[source,bash,role=execute]
----
podman exec -it automation-controller-task bash
awx-manage list_instances
----
+ 
[source,bash,role=execute]
----
exit
----

- Migration successful! Your new deployment is ready to go.
+
[source,bash,role=execute]
----
cd
cat external_ip
----
